{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Introduction to Deep Learning Coding Ex 4: Spam Message Generator!\n",
    "\n",
    "Contact T/A: Yeon-goon Kim, SNU ECE, CML. (ygoonkim@cml.snu.ac.kr)  \n",
    "\n",
    "Dataset from http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n",
    "\n",
    "On this homework, you will train spam message generator, which is basic RNN/LSTM/GRU char2char generation model. Of course, your results may not so good, but you can expect some sentence-like results by doing this homework sucessfully.\n",
    "\n",
    "## Now, We'll handle texts, not images. Is there any differences?\n",
    "\n",
    "Of course, there are many differences between processing images and texts. One is that text cannot be expressed directly in matrices or tensors. We know an image can be expressed in Tensor(n_channel, width, height). But how about an text sentence? Can word 'Homework' can be expressed in tensor directly? By what laws? With what shapes? Even if it can, which one is closer to the word, 'Burden', or 'Work'? This is called 'Word Embedding Problem' and be considered as one of the most important problem in Natural Language Process(NLP) resarch. Fortunatly, there are some generalized solution in this problem (though not prefect, anyway) and both Tensorflow and Pytorch give basic APIs that solve this problem automatically. You may use those APIs in this homework. \n",
    "\n",
    "The other one is that text is sequential data. Generally, when processing images, without batch, input is just one image. However in text, input is mostly some or one paragraphs/sentences, which is sequential data of embedded character or words. So, If we want to generate word 'Homework' with start token 'H', 'o' before 'H' and 'o' before 'Homew' should operate different when it gives to input. This is why we use RNN-based model in deep learning when processing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement-Tensorflow\n",
    "In this homework I recommend that you should use the latest \"anaconda\" stable version of Tensorflow, which is on now(2020-11-05) 2.2.x., but latest version(2.3.x) wouldn't be a serious problem. I'm using python3.7 on grading environment but there are no major changes on python3.6/8 so also will not be a serious problem. \n",
    "There are other required packages to run the code which is 'unidecode'. You can easily install these packages with 'pip install unidecode'.  \n",
    "\n",
    "You can add other packages if you want, but if they are not basically given pkgs in Python/Tensorflow you should contact T/A to check whether you can use them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Import Packages ##########\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import random\n",
    "import unidecode\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 2020\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "###### On TF2.0, it automatically select whether to use GPU(default) or CPU #####\n",
    "#USE_GPU = True\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "############################# Changeable Parameters #############################\n",
    "SEQ_LENGTH = 200\n",
    "N_ITER = 2500\n",
    "TXT_GEN_PERIOD = 500\n",
    "LEARNING_RATE = 0.005\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepration (Contact T/A If you wan to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
      "array([54, 74,  4,  4,  0, 94, 24, 27, 94, 49, 50, 74,  4,  4,  0, 94, 54,\n",
      "       14, 14, 94, 17, 14, 27, 77, 94, 32, 32, 32, 75, 54, 48, 54, 75, 10,\n",
      "       12, 76, 30, 76, 23, 10, 29,  2,  7,  0,  8,  1,  9,  8,  0, 94, 54,\n",
      "       55, 50, 51, 82, 94, 54, 14, 23, 13, 94, 54, 55, 50, 51, 94, 41, 53,\n",
      "       49, 39, 94, 29, 24, 94,  6,  2,  4,  6,  8, 96, 56, 53, 42, 40, 49,\n",
      "       55, 75, 94, 44, 22, 25, 24, 27, 29, 10, 23, 29, 94, 18, 23, 15, 24,\n",
      "       27, 22, 10, 29, 18, 24, 23, 94, 15, 24, 27, 94,  0,  2, 94, 30, 28,\n",
      "       14, 27, 75, 94, 55, 24, 13, 10, 34, 94, 18, 28, 94, 34, 24, 30, 27,\n",
      "       94, 21, 30, 12, 20, 34, 94, 13, 10, 34, 62, 94,  2, 94, 15, 18, 23,\n",
      "       13, 94, 24, 30, 29, 94, 32, 17, 34, 94, 73, 94, 21, 24, 16, 94, 24,\n",
      "       23, 29, 24, 94, 17, 29, 29, 25, 77, 76, 76, 32, 32, 32, 75, 30, 27,\n",
      "       10, 32, 18, 23, 23, 14, 27, 75, 12, 24, 22, 94, 29], dtype=int32)>, <tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
      "array([74,  4,  4,  0, 94, 24, 27, 94, 49, 50, 74,  4,  4,  0, 94, 54, 14,\n",
      "       14, 94, 17, 14, 27, 77, 94, 32, 32, 32, 75, 54, 48, 54, 75, 10, 12,\n",
      "       76, 30, 76, 23, 10, 29,  2,  7,  0,  8,  1,  9,  8,  0, 94, 54, 55,\n",
      "       50, 51, 82, 94, 54, 14, 23, 13, 94, 54, 55, 50, 51, 94, 41, 53, 49,\n",
      "       39, 94, 29, 24, 94,  6,  2,  4,  6,  8, 96, 56, 53, 42, 40, 49, 55,\n",
      "       75, 94, 44, 22, 25, 24, 27, 29, 10, 23, 29, 94, 18, 23, 15, 24, 27,\n",
      "       22, 10, 29, 18, 24, 23, 94, 15, 24, 27, 94,  0,  2, 94, 30, 28, 14,\n",
      "       27, 75, 94, 55, 24, 13, 10, 34, 94, 18, 28, 94, 34, 24, 30, 27, 94,\n",
      "       21, 30, 12, 20, 34, 94, 13, 10, 34, 62, 94,  2, 94, 15, 18, 23, 13,\n",
      "       94, 24, 30, 29, 94, 32, 17, 34, 94, 73, 94, 21, 24, 16, 94, 24, 23,\n",
      "       29, 24, 94, 17, 29, 29, 25, 77, 76, 76, 32, 32, 32, 75, 30, 27, 10,\n",
      "       32, 18, 23, 23, 14, 27, 75, 12, 24, 22, 94, 29, 17], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
      "array([94, 11, 14, 94, 17, 14, 27, 94, 15, 27, 18, 14, 23, 13, 75, 94, 53,\n",
      "       14, 25, 21, 34, 94, 60, 40, 54, 74,  4,  4,  0, 94, 24, 27, 94, 49,\n",
      "       50, 74,  4,  4,  0, 94, 54, 14, 14, 94, 17, 14, 27, 77, 94, 32, 32,\n",
      "       32, 75, 54, 48, 54, 75, 10, 12, 76, 30, 76, 23, 10, 29,  2,  7,  0,\n",
      "        8,  1,  9,  8,  0, 94, 54, 55, 50, 51, 82, 94, 54, 14, 23, 13, 94,\n",
      "       54, 55, 50, 51, 94, 41, 53, 49, 39, 94, 29, 24, 94,  6,  2,  4,  6,\n",
      "        8, 96, 56, 53, 42, 40, 49, 55, 75, 94, 44, 22, 25, 24, 27, 29, 10,\n",
      "       23, 29, 94, 18, 23, 15, 24, 27, 22, 10, 29, 18, 24, 23, 94, 15, 24,\n",
      "       27, 94,  0,  2, 94, 30, 28, 14, 27, 75, 94, 55, 24, 13, 10, 34, 94,\n",
      "       18, 28, 94, 34, 24, 30, 27, 94, 21, 30, 12, 20, 34, 94, 13, 10, 34,\n",
      "       62, 94,  2, 94, 15, 18, 23, 13, 94, 24, 30, 29, 94, 32, 17, 34, 94,\n",
      "       73, 94, 21, 24, 16, 94, 24, 23, 29, 24, 94, 17, 29], dtype=int32)>, <tf.Tensor: shape=(200,), dtype=int32, numpy=\n",
      "array([11, 14, 94, 17, 14, 27, 94, 15, 27, 18, 14, 23, 13, 75, 94, 53, 14,\n",
      "       25, 21, 34, 94, 60, 40, 54, 74,  4,  4,  0, 94, 24, 27, 94, 49, 50,\n",
      "       74,  4,  4,  0, 94, 54, 14, 14, 94, 17, 14, 27, 77, 94, 32, 32, 32,\n",
      "       75, 54, 48, 54, 75, 10, 12, 76, 30, 76, 23, 10, 29,  2,  7,  0,  8,\n",
      "        1,  9,  8,  0, 94, 54, 55, 50, 51, 82, 94, 54, 14, 23, 13, 94, 54,\n",
      "       55, 50, 51, 94, 41, 53, 49, 39, 94, 29, 24, 94,  6,  2,  4,  6,  8,\n",
      "       96, 56, 53, 42, 40, 49, 55, 75, 94, 44, 22, 25, 24, 27, 29, 10, 23,\n",
      "       29, 94, 18, 23, 15, 24, 27, 22, 10, 29, 18, 24, 23, 94, 15, 24, 27,\n",
      "       94,  0,  2, 94, 30, 28, 14, 27, 75, 94, 55, 24, 13, 10, 34, 94, 18,\n",
      "       28, 94, 34, 24, 30, 27, 94, 21, 30, 12, 20, 34, 94, 13, 10, 34, 62,\n",
      "       94,  2, 94, 15, 18, 23, 13, 94, 24, 30, 29, 94, 32, 17, 34, 94, 73,\n",
      "       94, 21, 24, 16, 94, 24, 23, 29, 24, 94, 17, 29, 29], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "with open('./spam.txt', 'r') as f:\n",
    "    textfile = f.read()\n",
    "\n",
    "TEXT_LENGTH = len(textfile)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "textfile = unidecode.unidecode(textfile)\n",
    "textfile = re.sub(' +',' ', textfile)\n",
    "\n",
    "\n",
    "def pick_input(textfile):\n",
    "    start_index = random.randint(0, TEXT_LENGTH - SEQ_LENGTH)\n",
    "    end_index = start_index + SEQ_LENGTH + 1\n",
    "    return textfile[start_index:end_index]\n",
    "\n",
    "def char2tensor(text):\n",
    "    lst = [string.printable.index(c) for c in text]\n",
    "    tensor = tf.Variable(lst)\n",
    "    return tensor\n",
    "\n",
    "def draw_random_sample(textfile):    \n",
    "    sampled_seq = char2tensor(pick_input(textfile))\n",
    "    inputs = sampled_seq[:-1]\n",
    "    outputs = sampled_seq[1:]\n",
    "    return inputs, outputs\n",
    "\n",
    "print(draw_random_sample(textfile))\n",
    "print(draw_random_sample(textfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Preparation Functions\n",
    "You can add any other functions that preparation data in below cell.  \n",
    "\n",
    "However, you should annotate precisely for each functions you define. One annotation line should not cover more than 5 lines that you write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: RNN/LSTM/GRU Module\n",
    "\n",
    "The main task is to create RNN/LSTM/GRU network. You can use any tensorflow/keras api that basically given.\n",
    "\n",
    "Basically build_model are given, but you can add other functions that help your networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "#################### WRITE DOWN YOUR CODE ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task: Train & Generate Code\n",
    "\n",
    "These cells would define functions of training network and generating text function. \n",
    "\n",
    "You can change these codes but if then you should annotate where do you make change precisely.\n",
    "One annotation line should not cover more than 5 lines that you make your changes.  \n",
    "Also, do not delete original code, just comment out them. (or make another cells of jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = 97,\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  rnn_units=HIDDEN_DIM,batch_size=1)\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "def accuracy(labels, logits):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(labels, logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam() \n",
    "model.compile(optimizer=optimizer, loss=loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  \n",
    "    num_generate = SEQ_LENGTH\n",
    "    input_eval = char2tensor(start_string)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = 0.8\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predictions = tf.math.exp(predictions)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(string.printable[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Code & Credit Creterion\n",
    "Half Credit (4 points): Generate some ugly text, without any meaningful words.\n",
    "\n",
    "Q3 Credits (6 points): In SEQ_LENGTH 200, generate 6 or less differet words.\n",
    "\n",
    "Full Credit (8 points): in SEQ_LENGTH 200, generate 7 or more different words.\n",
    "\n",
    "\n",
    "\n",
    "You can change this cell based on your code modifications above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './tf-checkpoints'+ datetime.datetime.now().strftime(\"_%Y.%m.%d-%H:%M:%S\")\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_0\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "joyd&H'E9~?nnz1Vf(It75%#++77|-gNTRUd\n",
      "=\\5@RQY('O-]3+\"xwdKi2D6@[Um }~N\n",
      "f#!07^`m785#!Gc&*uP?]\"tofCL~RDq+|l+Eucv,-;KX!@xK(L&9|X-VV,3k$95Cl]Lzg<d6U`5Z0^KbRxD8bX~$df\\8XZ47,,V^rQLQY|xOrmB=!'2dKuAo+V<{ 9bJ5f$\"]Z\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration 500\n",
      "joy or all 080000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1000\n",
      "joy or to 871870 from to 871870 prom a comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to comtome to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration 1500\n",
      "joyour a call 09066612112 from a call 09066612112 from a call 09066612112 from a call 09066612112 from a call 09066612112 from a call 09066612112 from a call 09066612112 from a call 09066612112 from a ca\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration 2000\n",
      "joyker for your call 0871230032 from land line only to claim call 0871230003 from land line only to claim call 0871230003 from land line only to claim call 0871230003 from land line only to claim call 08\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Training by running this cell\n",
    "for i in range(N_ITER):\n",
    "    x, y = draw_random_sample(textfile)    \n",
    "    history = model.fit(tf.expand_dims(x, 0), tf.expand_dims(y, 0), epochs=1, callbacks=[checkpoint_callback], verbose=0)\n",
    "    if (i % TXT_GEN_PERIOD == 0):\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_%d\"%i)\n",
    "            checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_prefix,\n",
    "                save_weights_only=True)\n",
    "            print(\"\\nIteration %d\"%i)\n",
    "            print(generate_text(model, 'joy'))\n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Code\n",
    "You can change this if you don't like or understand this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa6ada47f28>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(97,EMBEDDING_DIM,HIDDEN_DIM,batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(\"./tf-checkpoints_2020.11.29-17:21:52\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-trained Networks: Transformers\n",
    "\n",
    "Actually, RNN-based sequential model is now regarded as little bit old-fashioned, since 'Transformer' model was announced in paper 'Attention is All You Need'(https://arxiv.org/abs/1706.03762). Now this model is widely used on many state-of-the-art sequential-data-use model, and even in non-sequential-data-use (ex)image) model too. However, model training cost is too heavy(maybe you need multiple million-won GPUs) to train on this homework. Fortunately, there is package called 'transformers' that contains multipe pre-trained transformer-based model that can be used directly. Below is example of text generation using GPT2, which is one of the most popular pre-trained NLP models.\n",
    "\n",
    "You can install this package with 'pip install transformers'. To download pre-traind model, you may have 2GB or more free disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'my name is park and I really like it\" says Gabor Yevlikos, a local journalist. \"I went there this week because it\\'s a local market that I believe in. We used to run a bakery which sells biscuits from the local supermarket.\"\\n\\nThe local supermarket has a £10-a-meal menu and they\\'ve been putting a stop to many pies. So, they\\'re making some new ones because there\\'s no need to put them in the pies at the end of the day. \"I like that part now, but we don\\'t really want to be the only ones who use to be so busy. You don\\'t want to put it in a few pies as we think it does bring out the best in us.\"\\n\\nThe good news is they have a more efficient sales policy. The bad news, though, is they all run up a £10-a-meal menu - so it\\'s much quicker to order when it runs out. It\\'s not like there\\'s an \\'oh, there\\'s no one here\\' response, like in stores and supermarkets, with a \\'do not order\\' message or a \\'please don\\'t order\\' message as the shop is only closed two hours, in case it\\'s time to leave.\\n\\nSo, it\\'s not an ideal market, but it\\'s an important one and they\\'re doing something about it. \"We didn\\'t start thinking about our business about the store as a store,\" Gabor'},\n {'generated_text': 'my name is parkin\\' in the woods?\"\\n\\nThe girl shook her head. \"No.\"\\n\\n\"When we were on my date, I was just talking to my boyfriend. I thought he was having fun and wanted to play with us, so we played until we got up at midnight. And in a half hour, he was watching his phone. I think I would call him when I was at work. He would get in his car and start talking about the movies.\"\\n\\nHe was still talking in his girlfriend\\'s apartment, not quite able to get a good call, but it seemed that the woman was enjoying the fact that the night was finally over.\\n\\n\"He was like \\'well, you want two drinks and I\\'m not going to leave you so I\\'m not going to ask you anything tonight?\\' He was like \\'no, I don\\'t want two drinks, I\\'ll be waiting on your table, and you look good.\\'\"\\n\\nShe walked back to the car and dropped her shoes on the carpet and started walking with him out into the backyard.\\n\\n\"He\\'s such a cool guy,\" she told me. \"If I was playing, he\\'d always be out to get me. But it was like he was watching our date. I remember being at the bar on the night we met and he was just saying \\'hey, I\\'m not going to do any show, I\\'m really looking forward to it\\'. It was like getting him'}]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# Install ipywidgets and restart notebooks if you meet error message.\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(25)\n",
    "start_text = 'my name is park'\n",
    "generator(start_text, max_length=300, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: Follow Tutorial of Fine-tuned Networks (2 points.)\n",
    "There are hundreds of fine-tuned NLP models in 'transformers' package. Try one of these models and follow its tutorial (except language translation model). Results must produce some meaningful, or funny one, and you must write down what model you choose and explain its function (ex) what is input/output, what does it mean etc) with one or two sentences. \n",
    "\n",
    "Hint: You can find list of pre-trained models in 'transformers' package on https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions...\n",
      "\n",
      "Evaluating QA pairs...\n",
      "\n",
      "1) Q: How long did it take to get the Bitcoin to be sent to the address of his digital wallet?\n",
      "   A: \" On the official account of Mr Musk, the Tesla and SpaceX chief appeared to offer to double any Bitcoin payment sent to the address of his digital wallet \"for the next 30 minutes\". \n",
      "\n",
      "2) Q: How did the company fix the flaw?\n",
      "   A: Last year, Twitter chief executive Jack Dorsey's account was hacked, but the company said it had fixed the flaw that left his account vulnerable. \n",
      "\n",
      "3) Q: What did the campaign of Joe Biden say about the hack?\n",
      "   A: The campaign of Joe Biden, who is the current Democratic presidential candidate, said Twitter had \"locked down the account within a few minutes of the breach and removed the related tweet\". \n",
      "\n",
      "4) Q: What is the role of social media companies in the 2020 election?\n",
      "   A: \"Social media companies such as Twitter and, Facebook all have a duty to consider the damage and influence their platforms can have on the 2020 election, and I think some companies are taking that more seriously than others,\" she told the BBC. \n",
      "\n",
      "5) Q: How many followers do the targeted accounts have?\n",
      "   A: The Twitter accounts targeted have millions of followers. \n",
      "\n",
      "6) Q: What did the FBI say about the hack?\n",
      "   A: Twitter said the hackers had targeted its employees \"with access to internal systems and tools\". \n",
      "\n",
      "7) Q: How many tweets were deleted?\n",
      "   A: The tweets were deleted just minutes after they were first posted. \n",
      "\n",
      "8) Q: What is the real identity of the perpetrators?\n",
      "   A: \" In any case, the real identities of the perpetrators are as yet unknown. \n",
      "\n",
      "9) Q: What is the FBI's response to the hack?\n",
      "   A: The FBI has launched an investigation after hackers hijacked Twitter accounts of a number of high-profile US figures in an apparent Bitcoin scam. \n",
      "\n",
      "10) Q: What is the name of the hacker?\n",
      "    A: Cryptoforhealth is also a registered user name on Instagram, apparently set up contemporaneously to the hack. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bang/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "from question_generator.questiongenerator import QuestionGenerator, print_qa\n",
    "qg = QuestionGenerator()\n",
    "\n",
    "\n",
    "with open(\"question_generator/articles/twitter_hack.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "print_qa(qg.generate(text, num_questions=10, answer_style='sentences'))\n",
    "\n",
    "\n",
    "\n",
    "#################### WRITE DOWN YOUR CODE ################################\n",
    "\n",
    "########### WRITE DOWN YOUR Explaination with annotation #################\n",
    "\n",
    "# Explaination:\n",
    "# text를 input으로 받은 후에 그 text를 통하여 임의의 question과 answer를 generate합니다.\n",
    "\n",
    "\n",
    "##########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information: Project\n",
    "Of course, there are massive ammount of pretrained models on domain of image, NLP or else in web with open-source licenses. You can fine-tune those models if your GPUs are good enough, or at least transfer its information by using output feature of pre-trained networks. Or, maybe neither, it is up to you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}